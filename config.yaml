alpha: 0
batch_size: 128
beta: 0
bptt: 70
checkpoint: ''
data: ''
dataset: WT2
dropout_connect: 0.7
dropout_emb: 0.7
dropout_forward: 0.25
dropout_words: 0.1
emb_dim: 750
epochs: 800
gamma: 0
grad_clip: 0.1
hidden_dim: 1350
layers: 3
learning_rate: 0.001
learning_rate_thresholds: 0.0
log_interval: 1000
momentum: 0.0
num_parameters: 49180928
projection: false
prune: 0.0
pseudo_derivative_width: 3.6
rnn_type: egru
scheduler: cosine
scheduler_start: 400
scratch: ''
seed: 913420
thr_init_mean: 0.01
wandb: true
weight_decay: 0.12
weight_init_gain: 1.0
